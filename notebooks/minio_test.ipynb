{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed9388b-6da6-4a96-93c2-c57e188f6fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MinIO reachable (HTTP 403)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "try:\n",
    "    response = requests.get(\"http://minio:9000\", timeout=5)\n",
    "    print(f\"✅ MinIO reachable (HTTP {response.status_code})\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ MinIO connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6a9abf-8064-423b-8cfd-3c87365a0c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created with MinIO config:\n",
      "spark.hadoop.fs.s3a.access.key: minioadmin\n",
      "spark.hadoop.fs.s3a.attempts.maximum: 3\n",
      "spark.hadoop.fs.s3a.connection.establish.timeout: 5000\n",
      "spark.hadoop.fs.s3a.connection.ssl.enabled: false\n",
      "spark.hadoop.fs.s3a.connection.timeout: 10000\n",
      "spark.hadoop.fs.s3a.endpoint: http://minio:9000\n",
      "spark.hadoop.fs.s3a.fast.upload: true\n",
      "spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "spark.hadoop.fs.s3a.path.style.access: true\n",
      "spark.hadoop.fs.s3a.secret.key: minioadmin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"MinIO-Integration\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    # Performance and reliability settings:\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"10000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"3\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "    # Use local JARs instead of downloading:\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/*\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/*\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Verify configuration\n",
    "print(\"Spark session created with MinIO config:\")\n",
    "for (k,v) in sorted(spark.sparkContext._conf.getAll()):\n",
    "    if \"s3a\" in k.lower():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcd9fce-7c8e-4801-b93d-b397670e8ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files in bucket:\n",
      "s3a://default/data/smart_meter_data.json\n",
      "s3a://default/data/weather_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# List files in your bucket\n",
    "files = spark.sparkContext.wholeTextFiles(\"s3a://default/data/\")\n",
    "print(f\"Found {files.count()} files in bucket:\")\n",
    "for path in files.collect()[:3]:  # Show first 3 files\n",
    "    print(path[0])  # Prints S3 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34f9c33-183d-4964-88b1-caa1ebdd0109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting first 5 lines of raw JSON file:\n",
      "+-----------------------------------------------+\n",
      "|value                                          |\n",
      "+-----------------------------------------------+\n",
      "|[                                              |\n",
      "|  {                                            |\n",
      "|    \"meter_id\": \"WELLINGTON_626\",              |\n",
      "|    \"timestamp\": \"2025-06-10T06:10:07.414848Z\",|\n",
      "|    \"kwh_usage\": 3.54,                         |\n",
      "|    \"voltage\": 230,                            |\n",
      "|    \"customer_id\": \"CUST_7591\",                |\n",
      "|    \"region\": \"Christchurch\"                   |\n",
      "|  },                                           |\n",
      "|  {                                            |\n",
      "|    \"meter_id\": \"DUNEDIN_660\",                 |\n",
      "|    \"timestamp\": \"2025-06-10T21:25:07.414960Z\",|\n",
      "|    \"kwh_usage\": 2.14,                         |\n",
      "|    \"voltage\": 240,                            |\n",
      "|    \"customer_id\": \"CUST_1812\",                |\n",
      "|    \"region\": \"Auckland\"                       |\n",
      "|  },                                           |\n",
      "|  {                                            |\n",
      "+-----------------------------------------------+\n",
      "only showing top 18 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. First let's see the raw JSON content\n",
    "print(\"Inspecting first 5 lines of raw JSON file:\")\n",
    "raw_json = spark.read.text(\"s3a://default/data/smart_meter_data.json\")\n",
    "raw_json.show(18, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f9d027-b9df-48b4-bfb5-015744f02e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weather Data Analysis:\n",
      "Total records: 450\n",
      "\n",
      "Summary statistics:\n",
      "+-------+----------+----------+-----------------+-----------------+------------------+\n",
      "|summary|      date|    region|       max_temp_c|       min_temp_c|       rainfall_mm|\n",
      "+-------+----------+----------+-----------------+-----------------+------------------+\n",
      "|  count|       450|       450|              450|              450|               450|\n",
      "|   mean|      NULL|      NULL|17.53088888888889|7.988666666666666|10.167777777777786|\n",
      "| stddev|      NULL|      NULL|4.261962241501345|4.512712111506372| 5.806240935709952|\n",
      "|    min|2025-03-12|  Auckland|             10.0|              0.0|               0.0|\n",
      "|    max|2025-06-09|Wellington|             24.9|             15.0|              20.0|\n",
      "+-------+----------+----------+-----------------+-----------------+------------------+\n",
      "\n",
      "\n",
      "Average rainfall by region:\n",
      "+------------+------------------+------------------+-----------------+\n",
      "|      region|      avg_rainfall|      avg_max_temp|     avg_min_temp|\n",
      "+------------+------------------+------------------+-----------------+\n",
      "|  Wellington|10.500000000000002| 17.28444444444445|7.356666666666671|\n",
      "|Christchurch|10.003333333333332| 17.78111111111111|8.481111111111112|\n",
      "|    Auckland|10.985555555555557|16.635555555555552|8.146666666666667|\n",
      "|     Dunedin|  9.05111111111111| 18.07777777777778|8.154444444444442|\n",
      "|    Hamilton|10.298888888888888| 17.87555555555556|7.804444444444448|\n",
      "+------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Continue working with the weather data\n",
    "weather_df = spark.read.parquet(\"s3a://default/data/weather_data.parquet\")\n",
    "\n",
    "print(\"\\nWeather Data Analysis:\")\n",
    "print(f\"Total records: {weather_df.count()}\")\n",
    "print(\"\\nSummary statistics:\")\n",
    "weather_df.describe().show()\n",
    "\n",
    "print(\"\\nAverage rainfall by region:\")\n",
    "weather_df.groupBy(\"region\").agg(\n",
    "    avg(\"rainfall_mm\").alias(\"avg_rainfall\"),\n",
    "    avg(\"max_temp_c\").alias(\"avg_max_temp\"),\n",
    "    avg(\"min_temp_c\").alias(\"avg_min_temp\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39173823-f6fb-4e95-9ce3-4c77b2740f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
