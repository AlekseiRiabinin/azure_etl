{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd22e76-2c48-418d-9e67-f797d597c1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis notebook implements an end-to-end ETL pipeline that:\\n1. Reads streaming data from Kafka\\n2. Transforms the data with Spark\\n3. Writes results to PostgreSQL (for analytics) and MinIO (for archival)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This notebook implements an end-to-end ETL pipeline that:\n",
    "1. Reads streaming data from Kafka\n",
    "2. Transforms the data with Spark\n",
    "3. Writes results to PostgreSQL (for analytics) and MinIO (for archival)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b855fb09-bc5d-4e31-a0fa-7ff1b3509901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import psycopg2\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, current_timestamp, to_date, hour, dayofweek, when, lit, udf\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType, \n",
    "    DoubleType, IntegerType, BooleanType\n",
    ")\n",
    "from functools import lru_cache\n",
    "from typing import Any\n",
    "from time import sleep \n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_batch\n",
    "from psycopg2 import pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16010f97-b5fe-430a-a62d-a923915a1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with MinIO/S3 support\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"SmartMeterETL\")\n",
    "    \n",
    "    # JAR Configuration - Add Hadoop AWS and related jars\n",
    "    .config(\"spark.jars\", \",\".join([\n",
    "        \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar\",\n",
    "        \"/opt/spark/jars/kafka-clients-3.5.0.jar\",\n",
    "        \"/opt/spark/jars/kafka_2.12-3.5.0.jar\",\n",
    "        \"/opt/spark/jars/commons-pool2-2.11.1.jar\",\n",
    "        \"/opt/spark/jars/lz4-java-1.8.0.jar\",\n",
    "        \"/opt/spark/jars/snappy-java-1.1.10.1.jar\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar\",\n",
    "        \"/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "    ]))\n",
    "    \n",
    "    # Classpath Configuration\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/*\")\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/*\")\n",
    "    .config(\"spark.executor.userClassPathFirst\", \"true\")\n",
    "    \n",
    "    # MinIO/S3 Configuration\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9002\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    # MinIO/S3 Optimizations:\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\")\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload.active.blocks\", \"4\")\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.threshold\", \"128M\")\n",
    "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"100000\")\n",
    "\n",
    "    # Prevent temp files from appearing in MinIO/S3\n",
    "    .config(\"spark.hadoop.fs.s3a.committer.name\", \"directory\")\n",
    "    .config(\"spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a\", \n",
    "            \"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\")\n",
    "\n",
    "    # Kafka Specific Settings\n",
    "    .config(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\", \"false\")\n",
    "    .config(\"spark.kafka.consumer.cache.enabled\", \"false\")\n",
    "    .config(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "    .config(\"spark.streaming.kafka.maxRatePerPartition\", \"1000\")\n",
    "    \n",
    "    # JVM Options\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "           \"-Dio.netty.tryReflectionSetAccessible=true \" +\n",
    "           \"--add-opens=java.base/sun.nio.ch=ALL-UNNAMED \" +\n",
    "           \"--add-opens=java.base/java.lang=ALL-UNNAMED \" +\n",
    "           \"--add-opens=java.base/java.util=ALL-UNNAMED\")\n",
    "    \n",
    "    # Performance Tuning\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .config(\"spark.default.parallelism\", \"4\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    \n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb9014f-b657-41ad-98bf-53650c0ea6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kafka test connection successful\n"
     ]
    }
   ],
   "source": [
    "# Test Kafka connectivity before attempting to read\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "\n",
    "try:\n",
    "    # Simple test connection\n",
    "    test_df = (spark.read\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka-1:9092\")\n",
    "        .option(\"subscribe\", \"dummy\")\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load())\n",
    "    print(\"✅ Kafka test connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Kafka connection failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1829aeec-5ec1-4273-ada1-1b0018305a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Extract: Read Streaming Data from Kafka\n",
    "\n",
    "# Define schema for smart meter data\n",
    "meter_schema = StructType([\n",
    "    StructField(\"meter_id\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"kwh_usage\", DoubleType()),\n",
    "    StructField(\"voltage\", IntegerType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"region\", StringType())\n",
    "])\n",
    "\n",
    "kafka_df = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-1:9092,kafka-2:9095\")\n",
    "    .option(\"subscribe\", \"smart_meter_data\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"kafka.security.protocol\", \"PLAINTEXT\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .option(\"minPartitions\", \"1\")\n",
    "    .load())\n",
    "\n",
    "# Parse JSON data\n",
    "parsed_df = kafka_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), meter_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Debug\n",
    "debug_query = (parsed_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e38d33c0-78f3-4488-88af-c171846a5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Transform: Clean and Enrich Data\n",
    "\n",
    "# Define validation UDFs\n",
    "@udf(returnType=BooleanType())\n",
    "def is_valid_voltage(voltage: int) -> bool:\n",
    "    \"\"\"Check if voltage is valid (230V or 240V).\"\"\"\n",
    "    return voltage in [230, 240]\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def is_valid_kwh(kwh: float | int) -> bool:\n",
    "    \"\"\"Check if kWh usage is within reasonable bounds.\"\"\"\n",
    "    return 0 <= kwh <= 20\n",
    "\n",
    "# Transformation pipeline with proper parentheses\n",
    "enhanced_df = (parsed_df\n",
    "    # Current transformations\n",
    "    .withColumn(\"processing_time\", current_timestamp())\n",
    "    .withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "    .withColumn(\"hour_of_day\", hour(col(\"timestamp\")))\n",
    "    .withColumn(\"day_of_week\", dayofweek(col(\"timestamp\")))\n",
    "    .withColumn(\"cost\", \n",
    "        when(col(\"region\") == \"Auckland\", col(\"kwh_usage\") * 0.25)\n",
    "        .when(col(\"region\") == \"Wellington\", col(\"kwh_usage\") * 0.23)\n",
    "        .otherwise(col(\"kwh_usage\") * 0.20))\n",
    "    .withColumn(\"is_peak\", \n",
    "        ((col(\"hour_of_day\") >= 17) & (col(\"hour_of_day\") <= 21)))\n",
    "    \n",
    "    # Enhanced data quality checks\n",
    "    .withColumn(\"is_weekend\", col(\"day_of_week\").isin([1, 7]))\n",
    "    .withColumn(\"is_valid_voltage\", is_valid_voltage(col(\"voltage\")))\n",
    "    .withColumn(\"is_valid_kwh\", is_valid_kwh(col(\"kwh_usage\")))\n",
    "    .withColumn(\"data_quality_flag\",\n",
    "        when(col(\"is_valid_voltage\") & col(\"is_valid_kwh\"), \"VALID\")\n",
    "        .otherwise(\"INVALID\"))\n",
    "    \n",
    "    # Improved null handling\n",
    "    .filter(\n",
    "        col(\"meter_id\").isNotNull() & \n",
    "        col(\"customer_id\").isNotNull() &\n",
    "        col(\"timestamp\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    # Add record source\n",
    "    .withColumn(\"source_system\", lit(\"kafka_stream\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93323d95-fbcd-4dfd-b57b-f7bd3d4eb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Load: Write to Postgres\n",
    "\n",
    "# Function to write batch to PostgreSQL\n",
    "\n",
    "# Initialize connection pool\n",
    "postgres_pool = pool.SimpleConnectionPool(\n",
    "    minconn=1, maxconn=10,\n",
    "    host=\"postgres\", dbname=\"postgres\",\n",
    "    user=\"postgres\", password=\"postgres\"\n",
    ")\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_existing_customers(cur) -> set[str]:\n",
    "    \"\"\"Get all valid customer IDs with caching.\"\"\"\n",
    "    cur.execute(\"SELECT customer_id FROM dim_customer\")\n",
    "    return {row[0] for row in cur.fetchall()}\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_existing_meters(cur) -> set[str]:\n",
    "    \"\"\"Get all valid meter IDs with caching.\"\"\"\n",
    "    cur.execute(\"SELECT meter_id FROM dim_meter\")\n",
    "    return {row[0] for row in cur.fetchall()}\n",
    "\n",
    "def write_to_postgres(batch_df: DataFrame, batch_id: int) -> None:\n",
    "    \"\"\"Write batch data to Postgres with proper validation.\"\"\"\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "\n",
    "    batch_df.persist()\n",
    "    conn = None\n",
    "    \n",
    "    try:\n",
    "        pdf = batch_df.select([\n",
    "            \"meter_id\", \"timestamp\", \"kwh_usage\", \"voltage\",\n",
    "            \"customer_id\", \"region\", \"hour_of_day\", \"cost\",\n",
    "            \"is_peak\", \"is_weekend\", \"processing_time\",\n",
    "            \"date\", \"data_quality_flag\", \"source_system\"\n",
    "        ]).toPandas()\n",
    "\n",
    "        # Convert timestamps\n",
    "        pdf['timestamp'] = pd.to_datetime(pdf['timestamp'])\n",
    "        pdf['processing_time'] = pd.to_datetime(pdf['processing_time'])\n",
    "        pdf['date'] = pd.to_datetime(pdf['date']).dt.date\n",
    "\n",
    "        conn = postgres_pool.getconn()\n",
    "        with conn.cursor() as cur:\n",
    "            # Get existing references\n",
    "            existing_customers = get_existing_customers(cur)\n",
    "            existing_meters = get_existing_meters(cur)\n",
    "            \n",
    "            # Handle new meters\n",
    "            new_meters = set(pdf['meter_id']) - existing_meters\n",
    "            if new_meters:\n",
    "                print(f\"Batch {batch_id}: Adding {len(new_meters)} new meters\")\n",
    "                execute_batch(cur,\n",
    "                    \"\"\"INSERT INTO dim_meter (meter_id, installation_date, is_active)\n",
    "                       VALUES (%s, CURRENT_DATE, TRUE)\n",
    "                       ON CONFLICT (meter_id) DO NOTHING\"\"\",\n",
    "                    [(m,) for m in new_meters]\n",
    "                )\n",
    "            \n",
    "            # Filter out non-existent customers\n",
    "            valid_customers = pdf['customer_id'].isin(existing_customers)\n",
    "            invalid_customer_count = len(pdf) - sum(valid_customers)\n",
    "            if invalid_customer_count > 0:\n",
    "                print(f\"Batch {batch_id}: Filtering out {invalid_customer_count} invalid customer records\")\n",
    "                pdf = pdf[valid_customers]\n",
    "            \n",
    "            # Main insert with explicit column list\n",
    "            insert_sql = \"\"\"\n",
    "                INSERT INTO fact_smart_meter_readings (\n",
    "                    meter_id, timestamp, kwh_usage, voltage,\n",
    "                    customer_id, region, hour_of_day, cost,\n",
    "                    is_peak, is_weekend, processing_time,\n",
    "                    date, data_quality_flag, source_system\n",
    "                ) VALUES (\n",
    "                    %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "                    %s, %s, %s, %s, %s, %s\n",
    "                )\n",
    "                ON CONFLICT (meter_id, timestamp) \n",
    "                DO UPDATE SET\n",
    "                    kwh_usage = EXCLUDED.kwh_usage,\n",
    "                    voltage = EXCLUDED.voltage,\n",
    "                    cost = EXCLUDED.cost,\n",
    "                    data_quality_flag = EXCLUDED.data_quality_flag,\n",
    "                    processing_time = EXCLUDED.processing_time\n",
    "            \"\"\"\n",
    "            execute_batch(cur, insert_sql, \n",
    "                [tuple(row) for row in pdf.itertuples(index=False)],\n",
    "                page_size=100\n",
    "            )\n",
    "            conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {batch_id}: {str(e)}\")\n",
    "        if conn: conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        batch_df.unpersist()\n",
    "        if conn: postgres_pool.putconn(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d47c9f9-9873-4da4-afcf-16d2276ec761",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Load: Write to Minio/S3\n",
    "\n",
    "# Function to write batch to MinIO\n",
    "def write_to_minio(batch_df: DataFrame, batch_id: Any) -> None:\n",
    "    \"\"\"Write batch data to MinIO using pre-configured Spark session settings.\"\"\"\n",
    "    try:\n",
    "        if batch_df.isEmpty():\n",
    "            print(f\"Skipping empty batch {batch_id}\")\n",
    "            return\n",
    "       \n",
    "        # Use consistent path naming with date partitioning\n",
    "        output_path = f\"s3a://default/smart_meter/raw/batch_id={batch_id}/\"\n",
    "        \n",
    "        try:\n",
    "            (batch_df.write\n",
    "                .format(\"parquet\")\n",
    "                .mode(\"append\")\n",
    "                .option(\"compression\", \"snappy\")\n",
    "                .save(output_path))\n",
    "            \n",
    "            print(f\"Successfully wrote {batch_df.count()} records to {output_path}\")\n",
    "        except Exception as write_e:\n",
    "            print(f\"Failed to write batch {batch_id}: {str(write_e)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in batch {batch_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4ca97ec-1f88-4490-ae52-162dc3720347",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Execute the Streaming Pipeline\n",
    "\n",
    "def run_streaming():\n",
    "    \"\"\"Run batch processing ETL pipeline to MinIO and Postgres.\"\"\"\n",
    "    try:\n",
    "        print(\"Starting streaming queries...\")\n",
    "        \n",
    "        # Start PostgreSQL writer\n",
    "        pg_query = (enhanced_df.writeStream\n",
    "            .foreachBatch(write_to_postgres)\n",
    "            .option(\"checkpointLocation\", \"/tmp/checkpoints/postgres\")\n",
    "            .option(\"continueOnError\", \"true\")\n",
    "            .trigger(processingTime=\"30 seconds\")\n",
    "            .start())\n",
    "        \n",
    "        # Start MinIO writer\n",
    "        minio_query = (enhanced_df.writeStream\n",
    "            .foreachBatch(write_to_minio)\n",
    "            .option(\"checkpointLocation\", \"/tmp/checkpoints/minio\")\n",
    "            .option(\"continueOnError\", \"true\")\n",
    "            .option(\"maxFilesPerTrigger\", \"100\")\n",
    "            .option(\"maxOffsetsPerTrigger\", \"10000\")\n",
    "            .trigger(processingTime=\"30 seconds\")\n",
    "            .start())\n",
    "\n",
    "        print(\"Streaming queries started successfully\")\n",
    "        print(f\"PostgreSQL checkpoint: /tmp/checkpoints/postgres\")\n",
    "        print(f\"MinIO checkpoint: /tmp/checkpoints/minio\")\n",
    "        \n",
    "        # Handle each query separately\n",
    "        while True:\n",
    "            pg_status = pg_query.status\n",
    "            minio_status = minio_query.status\n",
    "            \n",
    "            # Status reporting\n",
    "            status_msg = [\n",
    "                \"\\n==== Pipeline Status ====\",\n",
    "                f\"PostgreSQL: {pg_status['message']} | Last Progress: {pg_query.lastProgress}\",\n",
    "                f\"MinIO: {minio_status['message']} | Last Progress: {minio_query.lastProgress}\",\n",
    "                \"=========================\"\n",
    "            ]\n",
    "            print(\"\\n\".join(status_msg))\n",
    "            \n",
    "            # Exception handling with detailed diagnostics\n",
    "            if pg_ex := pg_query.exception():\n",
    "                print(f\"\\nPOSTGRES ERROR: {str(pg_ex)}\")\n",
    "                print(\"Last 5 PostgreSQL batches:\")\n",
    "                for p in pg_query.recentProgress[-5:]:\n",
    "                    print(f\"- Batch {p['batchId']}: {p['numInputRows']} rows\")\n",
    "                \n",
    "            if minio_ex := minio_query.exception():\n",
    "                print(f\"\\nMINIO ERROR: {str(minio_ex)}\")\n",
    "                print(\"Last 5 MinIO batches:\")\n",
    "                for p in minio_query.recentProgress[-5:]:\n",
    "                    print(f\"- Batch {p['batchId']}: {p['numInputRows']} rows\")\n",
    "                \n",
    "            sleep(10)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nUser requested shutdown...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCRITICAL ERROR: {str(e)}\", file=sys.stderr)\n",
    "    finally:\n",
    "        print(\"\\nShutting down streams...\")\n",
    "        for name, q in [(\"PostgreSQL\", pg_query), (\"MinIO\", minio_query)]:\n",
    "            if q and q.isActive:\n",
    "                print(f\"Stopping {name} query...\")\n",
    "                try:\n",
    "                    q.stop()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error stopping {name} query: {str(e)}\")\n",
    "        print(\"All streams stopped\")\n",
    "    try:\n",
    "        with psycopg2.connect(\n",
    "            host=\"postgres\",\n",
    "            dbname=\"postgres\",\n",
    "            user=\"postgres\",\n",
    "            password=\"postgres\",\n",
    "            connect_timeout=5\n",
    "        ) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                # Create dim_customer if not exists\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS dim_customer (\n",
    "                        customer_id VARCHAR(50) PRIMARY KEY,\n",
    "                        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
    "                    )\n",
    "                \"\"\")\n",
    "                # Create dim_meter if not exists\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS dim_meter (\n",
    "                        meter_id VARCHAR(50) PRIMARY KEY,\n",
    "                        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
    "                    )\n",
    "                \"\"\")\n",
    "                conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing tables: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b694799-a569-477d-bfe5-a9370465047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting streaming queries...\n",
      "Streaming queries started successfully\n",
      "PostgreSQL checkpoint: /tmp/checkpoints/postgres\n",
      "MinIO checkpoint: /tmp/checkpoints/minio\n",
      "\n",
      "==== Pipeline Status ====\n",
      "PostgreSQL: Processing new data | Last Progress: None\n",
      "MinIO: Initializing sources | Last Progress: None\n",
      "=========================\n",
      "Batch 1: Adding 44 new meters\n",
      "Batch 1: Filtering out 27 invalid customer records\n",
      "Successfully wrote 364 records to s3a://default/smart_meter/raw/batch_id=4/\n",
      "\n",
      "==== Pipeline Status ====\n",
      "PostgreSQL: Processing new data | Last Progress: {'id': '9bfa51e9-f8fd-47ba-afe2-a02f84414361', 'runId': '8c9f2c7c-9fc7-4465-af40-a3ddaf150bf1', 'name': None, 'timestamp': '2025-07-23T20:39:51.352Z', 'batchId': 1, 'numInputRows': 53, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 14.0807651434644, 'durationMs': {'addBatch': 3602, 'commitOffsets': 99, 'getBatch': 0, 'queryPlanning': 53, 'triggerExecution': 3764}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 610, '1': 635, '3': 604, '0': 625}}, 'endOffset': {'smart_meter_data': {'2': 619, '1': 645, '3': 615, '0': 639}}, 'latestOffset': None, 'numInputRows': 53, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 14.0807651434644}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "MinIO: Processing new data | Last Progress: {'id': 'b5d1fb86-1e50-45a8-aa5f-2dce4ca73e7d', 'runId': '533642f6-c195-4a1f-8884-06061291c522', 'name': None, 'timestamp': '2025-07-23T20:39:51.389Z', 'batchId': 4, 'numInputRows': 823, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 107.23127035830619, 'durationMs': {'addBatch': 7345, 'commitOffsets': 125, 'getBatch': 1, 'latestOffset': 28, 'queryPlanning': 63, 'triggerExecution': 7675, 'walCommit': 100}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 3304, '1': 3286, '3': 3273, '0': 3312}}, 'endOffset': {'smart_meter_data': {'2': 3395, '1': 3372, '3': 3368, '0': 3404}}, 'latestOffset': {'smart_meter_data': {'2': 3395, '1': 3372, '3': 3368, '0': 3404}}, 'numInputRows': 823, 'inputRowsPerSecond': 0.0, 'processedRowsPerSecond': 107.23127035830619, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "=========================\n",
      "Batch 2: Adding 4070 new meters\n",
      "Batch 2: Filtering out 7575 invalid customer records\n",
      "Successfully wrote 17 records to s3a://default/smart_meter/raw/batch_id=5/\n",
      "\n",
      "==== Pipeline Status ====\n",
      "PostgreSQL: Waiting for next trigger | Last Progress: {'id': '9bfa51e9-f8fd-47ba-afe2-a02f84414361', 'runId': '8c9f2c7c-9fc7-4465-af40-a3ddaf150bf1', 'name': None, 'timestamp': '2025-07-23T20:40:00.001Z', 'batchId': 2, 'numInputRows': 13797, 'inputRowsPerSecond': 1595.213319458897, 'processedRowsPerSecond': 1288.7166075098075, 'durationMs': {'addBatch': 10277, 'commitOffsets': 25, 'getBatch': 0, 'latestOffset': 31, 'queryPlanning': 204, 'triggerExecution': 10706, 'walCommit': 162}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 619, '1': 645, '3': 615, '0': 639}}, 'endOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'latestOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'numInputRows': 13797, 'inputRowsPerSecond': 1595.213319458897, 'processedRowsPerSecond': 1288.7166075098075, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "MinIO: Waiting for next trigger | Last Progress: {'id': 'b5d1fb86-1e50-45a8-aa5f-2dce4ca73e7d', 'runId': '533642f6-c195-4a1f-8884-06061291c522', 'name': None, 'timestamp': '2025-07-23T20:40:00.001Z', 'batchId': 5, 'numInputRows': 40, 'inputRowsPerSecond': 4.644681839294008, 'processedRowsPerSecond': 3.9832702648874725, 'durationMs': {'addBatch': 9699, 'commitOffsets': 37, 'getBatch': 0, 'latestOffset': 7, 'queryPlanning': 167, 'triggerExecution': 10042, 'walCommit': 127}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 3395, '1': 3372, '3': 3368, '0': 3404}}, 'endOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'latestOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'numInputRows': 40, 'inputRowsPerSecond': 4.644681839294008, 'processedRowsPerSecond': 3.9832702648874725, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "=========================\n",
      "\n",
      "==== Pipeline Status ====\n",
      "PostgreSQL: Waiting for next trigger | Last Progress: {'id': '9bfa51e9-f8fd-47ba-afe2-a02f84414361', 'runId': '8c9f2c7c-9fc7-4465-af40-a3ddaf150bf1', 'name': None, 'timestamp': '2025-07-23T20:40:00.001Z', 'batchId': 2, 'numInputRows': 13797, 'inputRowsPerSecond': 1595.213319458897, 'processedRowsPerSecond': 1288.7166075098075, 'durationMs': {'addBatch': 10277, 'commitOffsets': 25, 'getBatch': 0, 'latestOffset': 31, 'queryPlanning': 204, 'triggerExecution': 10706, 'walCommit': 162}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 619, '1': 645, '3': 615, '0': 639}}, 'endOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'latestOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'numInputRows': 13797, 'inputRowsPerSecond': 1595.213319458897, 'processedRowsPerSecond': 1288.7166075098075, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "MinIO: Waiting for next trigger | Last Progress: {'id': 'b5d1fb86-1e50-45a8-aa5f-2dce4ca73e7d', 'runId': '533642f6-c195-4a1f-8884-06061291c522', 'name': None, 'timestamp': '2025-07-23T20:40:00.001Z', 'batchId': 5, 'numInputRows': 40, 'inputRowsPerSecond': 4.644681839294008, 'processedRowsPerSecond': 3.9832702648874725, 'durationMs': {'addBatch': 9699, 'commitOffsets': 37, 'getBatch': 0, 'latestOffset': 7, 'queryPlanning': 167, 'triggerExecution': 10042, 'walCommit': 127}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 3395, '1': 3372, '3': 3368, '0': 3404}}, 'endOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'latestOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'numInputRows': 40, 'inputRowsPerSecond': 4.644681839294008, 'processedRowsPerSecond': 3.9832702648874725, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "=========================\n",
      "\n",
      "==== Pipeline Status ====\n",
      "PostgreSQL: Processing new data | Last Progress: {'id': '9bfa51e9-f8fd-47ba-afe2-a02f84414361', 'runId': '8c9f2c7c-9fc7-4465-af40-a3ddaf150bf1', 'name': None, 'timestamp': '2025-07-23T20:40:00.001Z', 'batchId': 2, 'numInputRows': 13797, 'inputRowsPerSecond': 1595.213319458897, 'processedRowsPerSecond': 1288.7166075098075, 'durationMs': {'addBatch': 10277, 'commitOffsets': 25, 'getBatch': 0, 'latestOffset': 31, 'queryPlanning': 204, 'triggerExecution': 10706, 'walCommit': 162}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 619, '1': 645, '3': 615, '0': 639}}, 'endOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'latestOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'numInputRows': 13797, 'inputRowsPerSecond': 1595.213319458897, 'processedRowsPerSecond': 1288.7166075098075, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "MinIO: Processing new data | Last Progress: {'id': 'b5d1fb86-1e50-45a8-aa5f-2dce4ca73e7d', 'runId': '533642f6-c195-4a1f-8884-06061291c522', 'name': None, 'timestamp': '2025-07-23T20:40:00.001Z', 'batchId': 5, 'numInputRows': 40, 'inputRowsPerSecond': 4.644681839294008, 'processedRowsPerSecond': 3.9832702648874725, 'durationMs': {'addBatch': 9699, 'commitOffsets': 37, 'getBatch': 0, 'latestOffset': 7, 'queryPlanning': 167, 'triggerExecution': 10042, 'walCommit': 127}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 3395, '1': 3372, '3': 3368, '0': 3404}}, 'endOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'latestOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'numInputRows': 40, 'inputRowsPerSecond': 4.644681839294008, 'processedRowsPerSecond': 3.9832702648874725, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "=========================\n",
      "Batch 3: Adding 4 new meters\n",
      "Batch 3: Filtering out 35 invalid customer records\n",
      "Successfully wrote 54 records to s3a://default/smart_meter/raw/batch_id=6/\n",
      "\n",
      "==== Pipeline Status ====\n",
      "PostgreSQL: Waiting for next trigger | Last Progress: {'id': '9bfa51e9-f8fd-47ba-afe2-a02f84414361', 'runId': '8c9f2c7c-9fc7-4465-af40-a3ddaf150bf1', 'name': None, 'timestamp': '2025-07-23T20:40:30.001Z', 'batchId': 3, 'numInputRows': 62, 'inputRowsPerSecond': 2.066666666666667, 'processedRowsPerSecond': 15.503875968992247, 'durationMs': {'addBatch': 3634, 'commitOffsets': 42, 'getBatch': 0, 'latestOffset': 7, 'queryPlanning': 181, 'triggerExecution': 3999, 'walCommit': 129}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'endOffset': {'smart_meter_data': {'2': 3418, '1': 3390, '3': 3382, '0': 3420}}, 'latestOffset': {'smart_meter_data': {'2': 3418, '1': 3390, '3': 3382, '0': 3420}}, 'numInputRows': 62, 'inputRowsPerSecond': 2.066666666666667, 'processedRowsPerSecond': 15.503875968992247, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "MinIO: Waiting for next trigger | Last Progress: {'id': 'b5d1fb86-1e50-45a8-aa5f-2dce4ca73e7d', 'runId': '533642f6-c195-4a1f-8884-06061291c522', 'name': None, 'timestamp': '2025-07-23T20:40:30.001Z', 'batchId': 6, 'numInputRows': 116, 'inputRowsPerSecond': 3.8666666666666667, 'processedRowsPerSecond': 15.217106126197034, 'durationMs': {'addBatch': 7185, 'commitOffsets': 134, 'getBatch': 0, 'latestOffset': 7, 'queryPlanning': 168, 'triggerExecution': 7623, 'walCommit': 123}, 'stateOperators': [], 'sources': [{'description': 'KafkaV2[Subscribe[smart_meter_data]]', 'startOffset': {'smart_meter_data': {'2': 3400, '1': 3375, '3': 3374, '0': 3407}}, 'endOffset': {'smart_meter_data': {'2': 3418, '1': 3390, '3': 3382, '0': 3420}}, 'latestOffset': {'smart_meter_data': {'2': 3418, '1': 3390, '3': 3382, '0': 3420}}, 'numInputRows': 116, 'inputRowsPerSecond': 3.8666666666666667, 'processedRowsPerSecond': 15.217106126197034, 'metrics': {'avgOffsetsBehindLatest': '0.0', 'maxOffsetsBehindLatest': '0', 'minOffsetsBehindLatest': '0'}}], 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}\n",
      "=========================\n",
      "\n",
      "User requested shutdown...\n",
      "\n",
      "Shutting down streams...\n",
      "Stopping PostgreSQL query...\n",
      "Stopping MinIO query...\n",
      "All streams stopped\n"
     ]
    }
   ],
   "source": [
    "## 6. Run ETL\n",
    "run_streaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f04d122-92f0-45c0-b17b-9422614bb55f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
