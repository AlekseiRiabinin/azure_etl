#!/bin/bash

set -e  # Exit on any error

COMPOSE_FILE="docker-compose.azure.yml"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1"
}

# Step 1: Start core infrastructure services
echo "üöÄ Starting base infrastructure services..."
docker compose -f $COMPOSE_FILE up -d \
  postgres \
  kafka-1 \
  kafka-2 \
  spark-master \
  spark-worker \
  minio \
  jupyterlab \
  kafdrop \
  hive-metastore

# Step 2: Check Kafka brokers and create topics
log "‚è≥ Waiting for Kafka brokers to be ready..."
for broker in kafka-1:9092 kafka-2:9095; do
    container=$(echo $broker | cut -d':' -f1)
    port=$(echo $broker | cut -d':' -f2)
    
    MAX_WAIT=120
    WAITED=0
    INTERVAL=5
    
    while ! docker exec $container kafka-broker-api-versions.sh --bootstrap-server $container:$port >/dev/null 2>&1; do
        sleep $INTERVAL
        WAITED=$((WAITED + INTERVAL))
        log "Waiting for $container... ($WAITED/$MAX_WAIT seconds)"
        if [ "$WAITED" -ge "$MAX_WAIT" ]; then
            log "‚ùå Timed out waiting for $container"
            exit 1
        fi
    done
done
log "‚úÖ Both Kafka brokers are ready"

log "üìä Setting up Kafka topics..."

# topic:partitions:replicas
docker exec kafka-1 bash -c '
    topics=("smart_meter_data:4:2" "weather_data:2:2")
    
    for topic in "${topics[@]}"; do
        IFS=":" read -r name partitions replicas <<< "$topic"
        
        if ! kafka-topics.sh --describe --topic "$name" --bootstrap-server kafka-1:9092 >/dev/null 2>&1; then
            echo "[$(date "+%Y-%m-%d %H:%M:%S")] Creating topic: $name"
            kafka-topics.sh --create \
                --topic "$name" \
                --partitions "$partitions" \
                --replication-factor "$replicas" \
                --bootstrap-server kafka-1:9092,kafka-2:9095
        else
            echo "[$(date "+%Y-%m-%d %H:%M:%S")] Topic $name exists"
        fi
    done
'
log "‚úÖ Kafka topics ready"

# Step 3: Start Kafka Producer (after topics exist)
log "üöÄ Starting Kafka Producer..."
docker compose -f $COMPOSE_FILE up -d kafka-producer
log "‚úÖ Kafka Producer started (check logs with: docker logs -f kafka_producer)"

# Step 4: Wait for postgres to be healthy
log "‚è≥ Waiting for postgres to be ready..."
MAX_WAIT=120
WAITED=0
INTERVAL=5

until [ "$(docker inspect -f '{{.State.Health.Status}}' postgres 2>/dev/null || echo unhealthy)" == "healthy" ]; do
  sleep $INTERVAL
  WAITED=$((WAITED + INTERVAL))
  log "Still waiting for postgres ($WAITED sec elapsed)"
  if [ "$WAITED" -ge "$MAX_WAIT" ]; then
    log "‚ùå Timed out waiting for postgres."
    exit 1
  fi
done

# Step 5: Initialize Hive Metastore
HIVE_DB_NAME=hive_metastore
HIVE_CONTAINER=hive-metastore

log "üß™ Setting up Hive Metastore..."

# First ensure the database exists
DB_EXISTS=$(docker exec postgres psql -U postgres -d postgres -tAc "SELECT 1 FROM pg_database WHERE datname = '$HIVE_DB_NAME';")
if [ "$DB_EXISTS" != "1" ]; then
  log "üõ†Ô∏è Creating database $HIVE_DB_NAME..."
  docker exec postgres psql -U postgres -d postgres -c "CREATE DATABASE $HIVE_DB_NAME;"
  log "‚úÖ Database $HIVE_DB_NAME created."
fi

# Verify schema (entrypoint already initialized it)
log "üîç Verifying Hive Metastore schema..."
if docker exec $HIVE_CONTAINER schematool -dbType postgres -info; then
  log "‚úÖ Hive Metastore schema verified successfully"
else
  log "‚ùå Hive Metastore schema verification failed"
  docker logs $HIVE_CONTAINER
  exit 1
fi

# Step 6: Create 'default' bucket in MinIO
log "üì¶ Ensuring 'default' bucket exists in MinIO..."

set +e  # Temporarily disable exit-on-error
docker run --rm --network azure_etl_kafka-net minio/mc mc alias set local http://minio:9000 minioadmin minioadmin >/dev/null 2>&1
BUCKET_EXISTS=$(docker run --rm --network azure_etl_kafka-net minio/mc mc ls local 2>/dev/null | grep -c 'default')
set -e  # Re-enable strict mode

if [ "$BUCKET_EXISTS" -eq 0 ]; then
  log "ü™£  Creating MinIO bucket: 'default'"
  docker run --rm --network azure_etl_kafka-net minio/mc \
    mc alias set local http://minio:9000 minioadmin minioadmin && \
    docker run --rm --network azure_etl_kafka-net minio/mc \
      mc mb local/default
else
  log "‚úÖ MinIO bucket 'default' already exists."
fi

# Step 7: Initialize Airflow database (runs once with cleanup)
log "üîç Checking if Airflow DB is already initialized..."

INIT_CHECK=$(docker exec postgres psql -U postgres -d postgres -tAc "SELECT 1 FROM information_schema.tables WHERE table_name='dag';")

if [ "$INIT_CHECK" == "1" ]; then
  log "‚úÖ Airflow metadata DB already initialized. Skipping airflow-init."
else
  log "üîÑ Initializing Airflow metadata DB..."
  docker compose -f $COMPOSE_FILE up -d airflow-init
  docker compose -f $COMPOSE_FILE logs -f airflow-init
fi

# Step 8: Start Airflow webserver and scheduler
log "üöÄ Starting Airflow services..."
docker compose -f $COMPOSE_FILE up -d airflow-webserver airflow-scheduler

# Step 9: Wait until Airflow Webserver is responsive
log "‚è≥ Waiting for Airflow webserver to respond..."
MAX_WAIT=120
WAITED=0

until docker exec airflow-webserver curl -s localhost:8080 > /dev/null 2>&1; do
  sleep $INTERVAL
  WAITED=$((WAITED + INTERVAL))
  log "Still waiting for Airflow webserver ($WAITED sec elapsed)"
  if [ "$WAITED" -ge "$MAX_WAIT" ]; then
    log "‚ùå Timed out waiting for Airflow webserver."
    exit 1
  fi
done

# Step 10: Verify admin user creation (in case init step skipped it)
log "üë§ Verifying Airflow admin user..."
if ! docker exec airflow-webserver airflow users list | grep -q admin; then
  log "üë§ Creating Airflow admin user..."
  docker exec airflow-webserver airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
else
  log "‚úÖ Admin user already exists."
fi

# Step 11: Start Trino and DuckDB
log "üöÄ Starting Trino and DuckDB services..."

# More robust port checking with retries
PORT_CHECK_RETRIES=3
PORT_CHECK_DELAY=2
PORT_FREE=true

for ((i=1; i<=PORT_CHECK_RETRIES; i++)); do
    if ss -tuln | grep -q ':8090\b'; then
        log "‚ö†Ô∏è Port 8090 appears in use (attempt $i/$PORT_CHECK_RETRIES)"
        PORT_FREE=false
        sleep $PORT_CHECK_DELAY
    else
        PORT_FREE=true
        break
    fi
done

if ! $PORT_FREE; then
    log "‚ùå Port 8090 is in use according to multiple checks. Details:"
    ss -tulnp | grep ':8090\b' || true
    docker ps --format '{{.ID}}\t{{.Names}}\t{{.Ports}}' | grep '8090' || true
    log "Trying to identify and kill the conflicting process..."
    sudo lsof -i :8090 || true
    sudo kill -9 $(sudo lsof -t -i :8090) 2>/dev/null || true
    sleep 2
fi

# Final verification
if ss -tuln | grep -q ':8090\b'; then
    log "‚ùå Could not free port 8090 after cleanup attempts. Aborting."
    exit 1
fi

log "‚úÖ Port 8090 confirmed available after checks"

# Start DuckDB and Trino
docker compose -f "$COMPOSE_FILE" up -d duckdb trino-coordinator

# Step 12: Output access info
log "‚úÖ All services are up and running!"
log "‚û°Ô∏è  Access Airflow UI:     http://localhost:8083"
log "‚û°Ô∏è  Access Kafka UI:       http://localhost:9002"
log "‚û°Ô∏è  Access MinIO Console:  http://localhost:9001"
log "‚û°Ô∏è  Access JupyterLab:     http://localhost:8888"
log "‚û°Ô∏è  Access Trino UI:       http://localhost:8090"
log "‚û°Ô∏è  Access Spark UI:       http://localhost:8091"
log "üìå Kafka Producer Logs:    docker logs -f kafka_producer"

log "üìå Airflow Postgres Connection (if needed):"
log "  - Conn ID: azure_postgres"
log "  - Type: Postgres"
log "  - Host: postgres"
log "  - Port: 5432"
log "  - Username: postgres"
log "  - Password: postgres"
log "  - Schema: postgres"
